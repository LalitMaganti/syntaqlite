# Copyright 2025 The syntaqlite Authors. All rights reserved.
# Licensed under the Apache License, Version 2.0.

"""Generate custom token and keyword hash files for SQLite dialects.

This script allows external users to generate custom tokenizer data files
that can be injected into syntaqlite at compile time.

Usage:
    # First, set up the build environment:
    tools/dev/install-build-deps

    # Then generate custom files from your grammar extension:
    tools/generate-tokens --extend-grammar my_dialect.y --output out/my_dialect

    # To use the generated files, compile with:
    clang -c sqlite_tokenize.c \
        -DSYNTAQLITE_TOKENS_FILE=\"my_dialect/tokens.h\" \
        -DSYNTAQLITE_KEYWORDHASH_DATA_FILE=\"my_dialect/keywordhash_data.h\" \
        -I/path/to/syntaqlite
"""
import argparse
import re
import sys
import tempfile
from pathlib import Path

ROOT_DIR = Path(__file__).parent.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from python.sqlite_extractor import (
    ToolRunner,
    HeaderGenerator,
    SymbolRenameExact,
    SQLITE_BLESSING,
)
from python.sqlite_extractor.generators import extract_tk_defines


def parse_extension_keywords(grammar_path: Path) -> list[str]:
    """Parse %token declarations from an extension grammar file."""
    content = grammar_path.read_text()
    keywords = []
    for match in re.finditer(r'%token\s+([^%{]+?)(?=\n(?:%|\s*$)|$)', content, re.DOTALL):
        keywords.extend(re.findall(r'\b([A-Z][A-Z0-9_]*)\b', match.group(1)))
    # Dedupe preserving order
    return list(dict.fromkeys(keywords))


def generate_token_defs(
    runner: ToolRunner,
    output_path: Path,
    extension_grammar: Path,
) -> None:
    """Generate token definitions by merging extension grammar with SQLite's."""
    guard = "CUSTOM_TOKENS_H"

    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)

        # Concatenate: extension grammar + base grammar
        # Extension goes first because SQLite requires SPACE/COMMENT/ILLEGAL
        # to be the last tokens (they appear at the end of parse.y)
        base_grammar = runner.get_base_grammar()
        ext_grammar = extension_grammar.read_text()
        combined = ext_grammar + "\n" + base_grammar

        (tmpdir / "parse.y").write_text(combined)

        # Run lemon
        parse_h = runner.run_lemon(tmpdir / "parse.y")
        parse_h_content = parse_h.read_text()

        # Extract TK_* defines
        defines = extract_tk_defines(parse_h_content)

        # Generate header with usage instructions
        gen = HeaderGenerator(
            guard=guard,
            description=f"Token definitions generated from SQLite's parse.y via Lemon.\n** Generated by: tools/generate-tokens\n**\n** Use with syntaqlite by compiling with:\n**   -DSYNTAQLITE_TOKENS_FILE=\"{output_path.name}\"",
            regenerate_cmd="",  # Don't include regenerate cmd for external files
        )
        gen.write(output_path, "\n".join(defines))


def generate_keywordhash_data(
    runner: ToolRunner,
    output_path: Path,
    extra_keywords: list[str],
    tokens_include: str,
) -> None:
    """Generate keyword hash data file with extra keywords."""
    guard = "CUSTOM_KEYWORDHASH_DATA_H"

    # Run mkkeywordhash with extra keywords
    output = runner.run_mkkeywordhash(extra_keywords=extra_keywords)

    # Find start of actual generated content (skip the SQLite header comment)
    start_marker = "/* Hash score:"
    start = output.find(start_marker)
    if start == -1:
        start = 0

    generated = output[start:]

    # Find end of data section (before keywordCode function)
    keyword_code_marker = "static int keywordCode("
    keyword_code_start = generated.find(keyword_code_marker)
    if keyword_code_start == -1:
        print("Error: Could not find keywordCode function", file=sys.stderr)
        sys.exit(1)

    # Find the "Hash table decoded" comment that ends the data section
    hash_table_decoded = generated.rfind("/* Hash table decoded:", 0, keyword_code_start)
    if hash_table_decoded != -1:
        comment_end = generated.find("*/", hash_table_decoded) + 2
        data_section = generated[:comment_end]
    else:
        # Fallback: find SQLITE_N_KEYWORD define
        n_keyword = generated.find("#define SQLITE_N_KEYWORD")
        if n_keyword != -1:
            line_end = generated.find("\n", n_keyword) + 1
            data_section = generated[:line_end]
        else:
            data_section = generated[:keyword_code_start]

    # Rename keywordhash arrays to have syntaqlite_ prefix
    for symbol in ["zKWText", "aKWHash", "aKWNext", "aKWLen", "aKWOffset", "aKWCode"]:
        data_section = SymbolRenameExact(symbol, f"syntaqlite_{symbol}").apply(data_section)

    # Generate header with usage instructions
    header_content = f"""/* Include token definitions - aKWCode[] references TK_* values */
#include "{tokens_include}"

{data_section.strip()}
"""

    gen = HeaderGenerator(
        guard=guard,
        description=f"Keyword hash data generated from SQLite's mkkeywordhash.c\n** Generated by: tools/generate-tokens\n**\n** Use with syntaqlite by compiling with:\n**   -DSYNTAQLITE_KEYWORDHASH_DATA_FILE=\"{output_path.name}\"",
        regenerate_cmd="",
        includes=[],  # We handle the include manually above
    )
    gen.write(output_path, header_content)


def main():
    parser = argparse.ArgumentParser(
        description="Generate custom token and keyword hash files for SQLite dialects",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example:
    tools/generate-tokens --extend-grammar my_dialect.y --output out/my_dialect

Then compile with:
    clang -c sqlite_tokenize.c \\
        -DSYNTAQLITE_TOKENS_FILE=\\"my_dialect/tokens.h\\" \\
        -DSYNTAQLITE_KEYWORDHASH_DATA_FILE=\\"my_dialect/keywordhash_data.h\\" \\
        -I/path/to/syntaqlite
"""
    )
    parser.add_argument(
        "--extend-grammar", type=Path, required=True,
        help="Grammar file (.y) with %%token declarations for extra keywords"
    )
    parser.add_argument(
        "--output", type=Path, required=True,
        help="Output directory for generated files"
    )
    parser.add_argument(
        "--tokens-include", type=str, default="tokens.h",
        help="Include path for tokens.h in keywordhash_data.h (default: tokens.h)"
    )
    args = parser.parse_args()

    runner = ToolRunner(root_dir=ROOT_DIR)

    if not runner.sqlite_src.exists():
        print(f"SQLite not found at {runner.sqlite_src}.", file=sys.stderr)
        print("Run tools/dev/install-build-deps first.", file=sys.stderr)
        return 1

    if not args.extend_grammar.exists():
        print(f"Extension grammar not found: {args.extend_grammar}", file=sys.stderr)
        return 1

    # Parse %token declarations from extension grammar
    extra_keywords = parse_extension_keywords(args.extend_grammar)

    if not extra_keywords:
        print(f"Warning: No %token declarations found in {args.extend_grammar}", file=sys.stderr)
        print("Expected format: %token KEYWORD1 KEYWORD2 ...", file=sys.stderr)
        return 1

    print(f"Extension grammar: {args.extend_grammar}")
    print(f"  Extra keywords: {extra_keywords}")

    args.output.mkdir(parents=True, exist_ok=True)

    generate_token_defs(runner, args.output / "tokens.h", args.extend_grammar)
    generate_keywordhash_data(runner, args.output / "keywordhash_data.h", extra_keywords, args.tokens_include)

    print(f"\nDone! Generated files in {args.output}")
    print("\nTo use with syntaqlite, compile with:")
    print(f'  -DSYNTAQLITE_TOKENS_FILE=\\"{args.output}/tokens.h\\"')
    print(f'  -DSYNTAQLITE_KEYWORDHASH_DATA_FILE=\\"{args.output}/keywordhash_data.h\\"')
    return 0


if __name__ == "__main__":
    sys.exit(main())
