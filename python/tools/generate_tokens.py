# Copyright 2025 The syntaqlite Authors. All rights reserved.
# Licensed under the Apache License, Version 2.0.

"""Generate custom token and keyword hash files for SQLite dialects.

This script allows external users to generate custom tokenizer data files
that can be injected into syntaqlite at compile time.

Usage:
    # First, set up the build environment:
    tools/dev/install-build-deps

    # Then generate custom files from your grammar extension:
    tools/generate-tokens --extend-grammar my_dialect.y --output out/my_dialect

    # To use the generated files, compile with:
    clang -c sqlite_tokenize.c \
        -DSYNTAQLITE_TOKENS_FILE=\"my_dialect/tokens.h\" \
        -DSYNTAQLITE_KEYWORDHASH_DATA_FILE=\"my_dialect/keywordhash_data.h\" \
        -I/path/to/syntaqlite
"""
import argparse
import os
import re
import subprocess
import sys
import tempfile
from pathlib import Path

ROOT_DIR = Path(__file__).parent.parent.parent
SQLITE_SRC = ROOT_DIR / "third_party" / "src" / "sqlite" / "src"
SQLITE_TOOL = ROOT_DIR / "third_party" / "src" / "sqlite" / "tool"

# Dedicated build directory for this script
TOOLS_BUILD_DIR = ROOT_DIR / "out" / ".generate_tokens"

# SQLite's blessing comment for generated files
SQLITE_BLESSING = """**
** The author disclaims copyright to this source code.  In place of
** a legal notice, here is a blessing:
**
**    May you do good and not evil.
**    May you find forgiveness for yourself and forgive others.
**    May you share freely, never taking more than you give.
**
"""

# The last entry in SQLite's keyword table - we append after this
KEYWORD_TABLE_END = '''  { "WITHOUT",          "TK_WITHOUT",      ALWAYS,           1      },
};'''


def build_tool(name: str) -> Path:
    """Build a tool and return its path."""
    build_ninja = TOOLS_BUILD_DIR / "build.ninja"
    if not build_ninja.exists():
        print(f"Configuring build in {TOOLS_BUILD_DIR}...")
        TOOLS_BUILD_DIR.mkdir(parents=True, exist_ok=True)
        subprocess.run(
            [str(ROOT_DIR / "tools" / "dev" / "gn"), "gen", str(TOOLS_BUILD_DIR), "--args=is_debug=false"],
            capture_output=True, text=True,
        )
        if not build_ninja.exists():
            print("Failed to configure build", file=sys.stderr)
            sys.exit(1)

    print(f"Building {name}...")
    result = subprocess.run(
        [str(ROOT_DIR / "tools" / "dev" / "ninja"), "-C", str(TOOLS_BUILD_DIR), name],
        capture_output=True, text=True,
    )
    if result.returncode != 0:
        print(f"Build failed:\n{result.stderr}", file=sys.stderr)
        sys.exit(1)

    return TOOLS_BUILD_DIR / (f"{name}.exe" if os.name == "nt" else name)


def generate_keyword_table_end(extra_keywords: list[str]) -> str:
    """Generate the keyword table with extra keywords appended."""
    lines = ['  { "WITHOUT",          "TK_WITHOUT",      ALWAYS,           1      },']
    for kw in extra_keywords:
        # Pad to align with SQLite's formatting
        name_pad = 18 - len(kw) - 2  # Account for quotes
        token_pad = 15 - len(f"TK_{kw}") - 2
        lines.append(f'  {{ "{kw}",{" " * name_pad}"TK_{kw}",{" " * token_pad}ALWAYS,           1      }},')
    lines.append('};')
    return '\n'.join(lines)


def generate_token_defs(output_path: Path, extension_grammar: Path) -> None:
    """Generate token definitions file by concatenating grammars.

    Concatenates the extension grammar with SQLite's base grammar,
    then runs lemon to generate proper TK_ defines for all tokens
    used in rules.
    """
    guard = "CUSTOM_TOKENS_H"

    # Build lemon using ninja
    lemon_exe = build_tool("lemon")

    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)

        # Read base parse.y and extension grammar
        base_grammar = (SQLITE_SRC / "parse.y").read_text()
        ext_grammar = extension_grammar.read_text()

        # Concatenate: extension grammar + base grammar
        # Extension goes first because SQLite requires SPACE/COMMENT/ILLEGAL
        # to be the last tokens (they appear at the end of parse.y)
        combined = ext_grammar + "\n" + base_grammar

        # Write combined grammar
        (tmpdir / "parse.y").write_text(combined)
        (tmpdir / "lempar.c").write_bytes((SQLITE_TOOL / "lempar.c").read_bytes())

        # Run lemon
        print("Running lemon to generate token definitions...")
        result = subprocess.run(
            [str(lemon_exe), str(tmpdir / "parse.y")],
            cwd=tmpdir,
            capture_output=True, text=True
        )
        if result.returncode != 0:
            print(f"Lemon failed: {result.stderr}", file=sys.stderr)
            sys.exit(1)

        # Read generated parse.h and extract TK_* defines
        parse_h = (tmpdir / "parse.h").read_text()

        header = f"""/*
{SQLITE_BLESSING}** Token definitions generated from SQLite's parse.y via Lemon.
** Generated by: tools/generate-tokens
**
** Use with syntaqlite by compiling with:
**   -DSYNTAQLITE_TOKENS_FILE="{output_path.name}"
*/
#ifndef {guard}
#define {guard}

"""
        # Extract all #define TK_* lines
        defines = re.findall(r'^#define TK_\w+\s+\d+', parse_h, re.MULTILINE)

        footer = f"\n\n#endif /* {guard} */\n"

        output_path.write_text(header + "\n".join(defines) + footer)
        print(f"  Written: {output_path}")


def generate_keywordhash_data(extra_keywords: list[str], output_path: Path, tokens_include: str) -> None:
    """Generate keyword hash data file."""
    guard = "CUSTOM_KEYWORDHASH_DATA_H"

    # Build using ninja
    exe = build_tool("mkkeywordhash")

    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)

        # Modify mkkeywordhash.c to add extra keywords
        src = SQLITE_TOOL / "mkkeywordhash.c"
        content = src.read_text()
        replacement = generate_keyword_table_end(extra_keywords)
        content = content.replace(KEYWORD_TABLE_END, replacement)

        # Write modified source and compile
        tmp_src = tmpdir / "mkkeywordhash.c"
        tmp_src.write_text(content)
        tmp_exe = tmpdir / "mkkeywordhash"

        cc = os.environ.get("CC", "clang")
        print(f"Compiling mkkeywordhash with custom keywords...")
        result = subprocess.run(
            [cc, "-o", str(tmp_exe), str(tmp_src)],
            capture_output=True, text=True
        )
        if result.returncode != 0:
            print(f"Compile failed: {result.stderr}", file=sys.stderr)
            sys.exit(1)

        # Run and capture output
        print(f"Running mkkeywordhash...")
        result = subprocess.run([str(tmp_exe)], capture_output=True, text=True)
        if result.returncode != 0:
            print(f"Run failed: {result.stderr}", file=sys.stderr)
            sys.exit(1)

        output = result.stdout

        # Find start of actual generated content (skip the SQLite header comment)
        start_marker = "/* Hash score:"
        start = output.find(start_marker)
        if start == -1:
            start = 0

        generated = output[start:]

        # Find end of data section (before keywordCode function)
        keyword_code_marker = "static int keywordCode("
        keyword_code_start = generated.find(keyword_code_marker)
        if keyword_code_start == -1:
            print("Error: Could not find keywordCode function", file=sys.stderr)
            sys.exit(1)

        # Find the "Hash table decoded" comment that ends the data section
        hash_table_decoded = generated.rfind("/* Hash table decoded:", 0, keyword_code_start)
        if hash_table_decoded != -1:
            comment_end = generated.find("*/", hash_table_decoded) + 2
            data_section = generated[:comment_end]
        else:
            # Fallback: find SQLITE_N_KEYWORD define
            n_keyword = generated.find("#define SQLITE_N_KEYWORD")
            if n_keyword != -1:
                line_end = generated.find("\n", n_keyword) + 1
                data_section = generated[:line_end]
            else:
                data_section = generated[:keyword_code_start]

        # Write data file
        header = f"""/*
{SQLITE_BLESSING}** Keyword hash data generated from SQLite's mkkeywordhash.c
** Generated by: tools/generate-tokens
**
** Use with syntaqlite by compiling with:
**   -DSYNTAQLITE_KEYWORDHASH_DATA_FILE="{output_path.name}"
*/
#ifndef {guard}
#define {guard}

/* Include token definitions - aKWCode[] references TK_* values */
#include "{tokens_include}"

"""
        footer = f"\n#endif /* {guard} */\n"

        output_path.write_text(header + data_section.strip() + "\n" + footer)
        print(f"  Written: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Generate custom token and keyword hash files for SQLite dialects",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example:
    tools/generate-tokens --extend-grammar my_dialect.y --output out/my_dialect

Then compile with:
    clang -c sqlite_tokenize.c \\
        -DSYNTAQLITE_TOKENS_FILE=\\"my_dialect/tokens.h\\" \\
        -DSYNTAQLITE_KEYWORDHASH_DATA_FILE=\\"my_dialect/keywordhash_data.h\\" \\
        -I/path/to/syntaqlite
"""
    )
    parser.add_argument(
        "--extend-grammar", type=Path, required=True,
        help="Grammar file (.y) with %%token declarations for extra keywords"
    )
    parser.add_argument(
        "--output", type=Path, required=True,
        help="Output directory for generated files"
    )
    parser.add_argument(
        "--tokens-include", type=str, default="tokens.h",
        help="Include path for tokens.h in keywordhash_data.h (default: tokens.h)"
    )
    args = parser.parse_args()

    if not SQLITE_SRC.exists():
        print(f"SQLite not found at {SQLITE_SRC}.", file=sys.stderr)
        print("Run tools/dev/install-build-deps first.", file=sys.stderr)
        return 1

    if not args.extend_grammar.exists():
        print(f"Extension grammar not found: {args.extend_grammar}", file=sys.stderr)
        return 1

    # Parse %token declarations from extension grammar
    ext_content = args.extend_grammar.read_text()
    keywords = []
    for match in re.finditer(r'%token\s+([^%{]+?)(?=\n(?:%|\s*$)|$)', ext_content, re.DOTALL):
        keywords.extend(re.findall(r'\b([A-Z][A-Z0-9_]*)\b', match.group(1)))
    extra_keywords = list(dict.fromkeys(keywords))  # dedupe preserving order

    if not extra_keywords:
        print(f"Warning: No %token declarations found in {args.extend_grammar}", file=sys.stderr)
        print("Expected format: %token KEYWORD1 KEYWORD2 ...", file=sys.stderr)
        return 1

    print(f"Extension grammar: {args.extend_grammar}")
    print(f"  Extra keywords: {extra_keywords}")

    args.output.mkdir(parents=True, exist_ok=True)

    generate_token_defs(args.output / "tokens.h", args.extend_grammar)
    generate_keywordhash_data(extra_keywords, args.output / "keywordhash_data.h", args.tokens_include)

    print(f"\nDone! Generated files in {args.output}")
    print("\nTo use with syntaqlite, compile with:")
    print(f'  -DSYNTAQLITE_TOKENS_FILE=\\"{args.output}/tokens.h\\"')
    print(f'  -DSYNTAQLITE_KEYWORDHASH_DATA_FILE=\\"{args.output}/keywordhash_data.h\\"')
    return 0


if __name__ == "__main__":
    sys.exit(main())
